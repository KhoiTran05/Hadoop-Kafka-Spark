ARG AIRFLOW_VERSION=2.8.4
ARG PYTHON_VERSION=3.10
FROM apache/airflow:${AIRFLOW_VERSION}-python${PYTHON_VERSION}

USER root
RUN apt-get update && apt-get install -y \
    build-essential \
    gcc \
    g++ \
    make \
    libkrb5-dev \
    libsasl2-dev \
    libssl-dev \
    libffi-dev \
    libpq-dev \
    python3-dev \
    default-libmysqlclient-dev \
    unixodbc-dev \
    curl \
    default-jdk \
    wget \
    gpg \
    apt-transport-https \
    && rm -rf /var/lib/apt/lists/*

# === JAVA 8 (TEMURIN) ===
RUN set -ex; \
    wget -O - https://packages.adoptium.net/artifactory/api/gpg/key/public | gpg --dearmor | tee /usr/share/keyrings/adoptium.gpg > /dev/null; \
    echo "deb [signed-by=/usr/share/keyrings/adoptium.gpg] https://packages.adoptium.net/artifactory/deb $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/adoptium.list; \
    apt-get update; \
    apt-get install -y temurin-8-jdk; \
    apt-get clean
    
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 
ENV PATH=$JAVA_HOME/bin:$PATH

# ==== Hadoop ====
ARG HADOOP_VERSION=3.3.6
COPY hadoop-${HADOOP_VERSION}.tar.gz /tmp/hadoop.tar.gz
RUN set -ex; \
    tar -xzf /tmp/hadoop.tar.gz -C /opt/; \
    rm /tmp/hadoop.tar.gz; \
    ln -s /opt/hadoop-$HADOOP_VERSION /opt/hadoop; \
    mkdir -p /opt/hadoop/etc/hadoop

# ==== Hadoop core-site.xml ====
RUN echo '<?xml version="1.0" encoding="UTF-8"?>' > /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '<configuration>' >> /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '  <property>' >> /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '    <name>fs.defaultFS</name>' >> /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '    <value>hdfs://namenode:9000</value>' >> /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '  </property>' >> /opt/hadoop/etc/hadoop/core-site.xml && \
    echo '</configuration>' >> /opt/hadoop/etc/hadoop/core-site.xml

# ==== Spark ====
ARG SPARK_VERSION=3.5.0
COPY spark-${SPARK_VERSION}-bin-hadoop3.tgz /tmp/spark.tgz
RUN set -ex; \
    tar -xzf /tmp/spark.tgz -C /opt/; \
    rm /tmp/spark.tgz; \
    ln -s /opt/spark-$SPARK_VERSION-bin-hadoop3 /opt/spark

# ==== KAFKA INTEGRATION FOR SPARK ====
RUN cd /opt/spark/jars && \
    wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar && \
    wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar && \
    wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar && \
    wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar

# ==== Hive ====
ARG HIVE_VERSION=2.3.2
COPY apache-hive-$HIVE_VERSION-bin.tar.gz /tmp/hive.tar.gz
RUN set -ex; \
    tar -xzf /tmp/hive.tar.gz -C /opt/; \
    rm /tmp/hive.tar.gz; \
    ln -s /opt/apache-hive-$HIVE_VERSION-bin /opt/hive; \
    mkdir -p /opt/hive/conf

USER airflow

# ==== Python dependencies for Airflow ====
COPY infrastructure/docker/requirements_airflow.txt /requirements_airflow.txt

RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir \
        apache-airflow-providers-postgres==5.10.1 \
        apache-airflow-providers-apache-hive==5.1.0 \
        apache-airflow-providers-apache-hdfs==4.3.0 \
        apache-airflow-providers-cncf-kubernetes \
        pyspark==3.5.0 && \
    pip install --no-cache-dir -r /requirements_airflow.txt

ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
ENV SPARK_HOME=/opt/spark
ENV SPARK_CONF_DIR=/opt/spark/conf
ENV HIVE_HOME=/opt/hive
ENV HIVE_CONF_DIR=/opt/hive/conf

ENV PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin:$HIVE_HOME/bin:$HIVE_HOME/sbin

WORKDIR /opt/airflow

USER airflow
